{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import IPython.display as display\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    # in order to use CNN, add one additional dimension \n",
    "    # to the original image\n",
    "    # img shape: [height, width, channel] -> [batch_size, height, width, channel]\n",
    "    img = img[tf.newaxis, :]\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = './dataset/content_nthu.jpg'\n",
    "content_image = load_img(content_path)\n",
    "print('Image shape:', content_image.shape)\n",
    "imshow(content_image, 'Content Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pretrained network (VGG19)\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/11-2_Visualization_Style-Transfer/figs/vgg19.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "\n",
    "# load pretrained network(VGG19)\n",
    "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "prediction_probabilities = vgg(x)\n",
    "prediction_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
    "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize filter shapes\n",
    "for layer in vgg.layers:\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "# retrieve weights from the second hidden layer\n",
    "filters, biases = vgg.layers[1].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "# plot first few filters\n",
    "n_filters, ix = 64, 1\n",
    "\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(14, 14, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "# redefine model to output right after the first hidden layer\n",
    "model = tf.keras.Model(inputs=[vgg.input], outputs=vgg.layers[1].output)\n",
    "model.summary()\n",
    "\n",
    "# preprocess input\n",
    "content_image = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "content_image = tf.image.resize(content_image, (224, 224))\n",
    "\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(content_image)\n",
    "\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "square = 8\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature maps for last convolutional layer in each block\n",
    "ixs = [2, 5, 10, 15, 20]\n",
    "outputs = [vgg.layers[i].output for i in ixs]\n",
    "model = tf.keras.Model(inputs=[vgg.input], outputs=outputs)\n",
    "feature_maps = model.predict(content_image)\n",
    "\n",
    "# plot the output from each block\n",
    "square = 8\n",
    "for i, fmap in enumerate(feature_maps):\n",
    "    # plot all 64 maps in an 8x8 squares\n",
    "    ix = 1\n",
    "    print(outputs[i])\n",
    "    plt.figure(figsize=(16,16))\n",
    "    for _ in range(square):\n",
    "        for _ in range(square):\n",
    "            # specify subplot and turn of axis\n",
    "            ax = pyplot.subplot(square, square, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # plot filter channel in grayscale\n",
    "            pyplot.imshow(fmap[0, :, :, ix-1], cmap='gray')\n",
    "            ix += 1\n",
    "            \n",
    "    # show the figure\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
    "    # Load our model. Load pretrained VGG, trained on imagenet data\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "\n",
    "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientModel(tf.keras.models.Model):\n",
    "    def __init__(self, layers):\n",
    "        super(GradientModel, self).__init__()\n",
    "        self.vgg =  vgg_layers(layers)\n",
    "        self.num_style_layers = len(layers)\n",
    "        self.vgg.trainable = False\n",
    "        \n",
    "    # return the feature map of required layer\n",
    "    def call(self, inputs):\n",
    "        \"Expects float input in [0,1]\"\n",
    "        inputs = inputs*255.0\n",
    "        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "        outputs = self.vgg(preprocessed_input)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def visualize_gradient(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        feature = extractor(image)\n",
    "        # grad = d_feature/d_image\n",
    "        grad = tape.gradient(tf.reduce_max(feature, axis=3), image)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = load_img(content_path)\n",
    "\n",
    "# activation layer\n",
    "layers = ['block4_conv2']\n",
    "image = tf.Variable(content_image)\n",
    "\n",
    "extractor = GradientModel(layers)\n",
    "grad = visualize_gradient(image)\n",
    "\n",
    "# look at the range of gradients\n",
    "print(\"shape: \", grad.numpy().shape)\n",
    "print(\"min: \", grad.numpy().min())\n",
    "print(\"max: \", grad.numpy().max())\n",
    "print(\"mean: \", grad.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize filter values to 0-1 so we can visualize them\n",
    "g_min, g_max = grad.numpy().min(), grad.numpy().max()\n",
    "filters = (grad - g_min) / (g_max - g_min)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(image.read_value()[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(filters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def visualize_gradient_single_layer(image, layer_i):\n",
    "    with tf.GradientTape() as tape:\n",
    "        feature = extractor(image)\n",
    "        grad = tape.gradient(tf.reduce_mean(feature[:, :, :, layer_i]), image)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "grad = visualize_gradient_single_layer(image, 77)\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "g_min, g_max = grad.numpy().min(), grad.numpy().max()\n",
    "filters = (grad - g_min) / (g_max - g_min)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(image.read_value()[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(filters[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided-Backpropagation\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/11-2_Visualization_Style-Transfer/figs/visualization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guided_backprop import GuidedBackprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "\n",
    "# backprop_vgg = GuidedBackprop(model=vgg, layerName='predictions') # original\n",
    "backprop_vgg = GuidedBackprop(model=vgg, layerName='block5_conv4') # use this layer instead, b/c we need to extract from vgg19.\n",
    "grad = backprop_vgg.guided_backprop(x)[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the original image and the three saliency map variants\n",
    "plt.figure(figsize=(16, 16), facecolor='w')\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Input')\n",
    "plt.imshow(tf.image.resize(content_image, (224, 224))[0])\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Abs. saliency')\n",
    "plt.imshow(np.abs(grad).max(axis=-1), cmap='gray')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Pos. saliency')\n",
    "plt.imshow((np.maximum(0, grad) / grad.max()))\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('Neg. saliency')\n",
    "plt.imshow((np.maximum(0, -grad) / -grad.min()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Neural Algorithm of Artistic Style\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/11-2_Visualization_Style-Transfer/dataset/nthu_candy.jpg)\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/11-2_Visualization_Style-Transfer/dataset/style_transfer_nthu_candy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define content and style representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = './dataset/content_nthu.jpg'\n",
    "style_path = './dataset/style_starry_night.jpg'\n",
    "\n",
    "content_image = load_img(content_path)\n",
    "style_image = load_img(style_path)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(content_image, 'Content Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(style_image, 'Style Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "print()\n",
    "for layer in vgg.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content layer where will pull our feature maps\n",
    "content_layers = ['block5_conv2'] \n",
    "\n",
    "# Style layer of interest\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1', \n",
    "                'block4_conv1', \n",
    "                'block5_conv1']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
    "    # Load our model. Load pretrained VGG, trained on imagenet data\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "\n",
    "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_extractor = vgg_layers(style_layers)\n",
    "style_outputs = style_extractor(style_image*255)\n",
    "\n",
    "#Look at the statistics of each layer's output\n",
    "for name, output in zip(style_layers, style_outputs):\n",
    "    print(name)\n",
    "    print(\"  shape: \", output.numpy().shape)\n",
    "    print(\"  min: \", output.numpy().min())\n",
    "    print(\"  max: \", output.numpy().max())\n",
    "    print(\"  mean: \", output.numpy().mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "    return result/(num_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract style and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleContentModel(tf.keras.models.Model):\n",
    "    def __init__(self, style_layers, content_layers):\n",
    "        super(StyleContentModel, self).__init__()\n",
    "        self.vgg =  vgg_layers(style_layers + content_layers)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.num_style_layers = len(style_layers)\n",
    "        self.vgg.trainable = False\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"Expects float input in [0,1]\"\n",
    "        inputs = inputs*255.0\n",
    "        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "        outputs = self.vgg(preprocessed_input)\n",
    "        style_outputs, content_outputs = (outputs[:self.num_style_layers], \n",
    "                                          outputs[self.num_style_layers:])\n",
    "\n",
    "        style_outputs = [gram_matrix(style_output)\n",
    "                         for style_output in style_outputs]\n",
    "\n",
    "        content_dict = {content_name:value \n",
    "                        for content_name, value \n",
    "                        in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "        style_dict = {style_name:value\n",
    "                      for style_name, value\n",
    "                      in zip(self.style_layers, style_outputs)}\n",
    "\n",
    "        return {'content':content_dict, 'style':style_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = StyleContentModel(style_layers, content_layers)\n",
    "\n",
    "results = extractor(tf.constant(content_image))\n",
    "\n",
    "style_results = results['style']\n",
    "\n",
    "print('Styles:')\n",
    "for name, output in sorted(results['style'].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())\n",
    "    print()\n",
    "\n",
    "print(\"Contents:\")\n",
    "for name, output in sorted(results['content'].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/11-2_Visualization_Style-Transfer/figs/fig-style-transfer-algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs['style']\n",
    "    content_outputs = outputs['content']\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
    "                           for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / num_style_layers\n",
    "\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
    "                             for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / num_content_layers\n",
    "    loss = style_loss + content_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_0_1(image):\n",
    "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "\n",
    "    # tape.gradient: d_loss/d_image\n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "style_weight = 2    # Change it as you want\n",
    "content_weight = 10  # Change it as you want\n",
    "style_targets = extractor(style_image)['style']\n",
    "content_targets = extractor(content_image)['content']\n",
    "\n",
    "image = tf.Variable(content_image)\n",
    "\n",
    "\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "plt.imshow(image.read_value()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(image)\n",
    "    imshow(image.read_value())\n",
    "    plt.title(\"Train step: {}\".format(step))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total variation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "sobel = tf.image.sobel_edges(content_image)\n",
    "plt.subplot(1,2,1)\n",
    "imshow(clip_0_1(sobel[...,0]/4+0.5), \"Horizontal Sobel-edges\")\n",
    "plt.subplot(1,2,2)\n",
    "imshow(clip_0_1(sobel[...,1]/4+0.5), \"Vertical Sobel-edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V(y)=\\sum_i \\sum_j\\sqrt{(y_{i+1,j}-y_{i,j})^2 + (y_{i,j+1}-y_{i,j})^2}$$\n",
    "\n",
    "$$V(y)=\\sum_i \\sum_j|y_{i+1,j}-y_{i,j}| + |y_{i,j+1}-y_{i,j}|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(image):\n",
    "    # TODO\n",
    "    # height = image.shape[1]\n",
    "    # width = image.shape[2]\n",
    "    # loss = 0\n",
    "    # for i in range(height - 1):\n",
    "    #     for j in range(width - 1):\n",
    "    #         loss += abs(image[:, i + 1, j, :] - image[:, i, j, :]) + abs(image[:, i, j + 1, :] - image[:, i, j, :])\n",
    "\n",
    "    # image_np = image.numpy()\n",
    "    \n",
    "    # diff1 = np.abs(image_np[:, 1:, :, :] - image_np[:, :-1, :, :])\n",
    "    # diff2 = np.abs(image_np[:, :, 1:, :] - image_np[:, :, :-1, :])\n",
    "    \n",
    "    # loss = np.sum(diff1) + np.sum(diff2)\n",
    "    \n",
    "    diff_v = tf.reduce_sum(tf.abs(image[:, 1:, :, :] - image[:, :-1, :, :]))\n",
    "    diff_h = tf.reduce_sum(tf.abs(image[:, :, 1:, :] - image[:, :, :-1, :]))\n",
    "    \n",
    "    loss = diff_v + diff_h\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variation_weight = 20 # Change it as you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "        loss += total_variation_weight*total_variation_loss(image)\n",
    "\n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(content_image)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(image)\n",
    "    imshow(image.read_value())\n",
    "    plt.title(\"Train step: {}\".format(step))\n",
    "    plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './dataset/style_transfer_nthu_starry_night.png'\n",
    "mpl.image.imsave(file_name, image[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaIN\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/11-2_Visualization_Style-Transfer/figs/adain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "CONTENT_DIRS = ['./dataset/mscoco/test2014']\n",
    "STYLE_DIRS = ['./dataset/wikiart/test']\n",
    "\n",
    "# VGG19 was trained by Caffe which converted images from RGB to BGR,\n",
    "# then zero-centered each color channel with respect to the ImageNet \n",
    "# dataset, without scaling.  \n",
    "IMG_MEANS = np.array([103.939, 116.779, 123.68]) # BGR\n",
    "\n",
    "IMG_SHAPE = (224, 224, 3) # training image shape, (h, w, c)\n",
    "SHUFFLE_BUFFER = 1000\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 30\n",
    "STEPS_PER_EPOCH = 12000 // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_files(dir, num, pattern='**/*.jpg'):\n",
    "    '''Samples files in a directory using the reservoir sampling.'''\n",
    "\n",
    "    paths = Path(dir).glob(pattern) # list of Path objects\n",
    "    sampled = []\n",
    "    for i, path in enumerate(paths):\n",
    "        if i < num:\n",
    "            sampled.append(path) \n",
    "        else:\n",
    "            s = random.randint(0, i)\n",
    "            if s < num:\n",
    "                sampled[s] = path\n",
    "    return sampled\n",
    "\n",
    "def plot_images(dir, row, col, pattern):\n",
    "    paths = sample_files(dir, row*col, pattern)\n",
    "\n",
    "    plt.figure(figsize=(2*col, 2*row))\n",
    "    for i in range(row*col):\n",
    "        im = Image.open(paths[i])\n",
    "        w, h = im.size\n",
    "\n",
    "        plt.subplot(row, col, i+1)\n",
    "        plt.imshow(im)\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlabel(f'{w}x{h}')\n",
    "    plt.show()\n",
    "\n",
    "print('Sampled content images:')\n",
    "plot_images(CONTENT_DIRS[0], 4, 8, pattern='*.jpg')\n",
    "\n",
    "print('Sampled style images:')\n",
    "plot_images(STYLE_DIRS[0], 4, 8, pattern='*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(dir_path, min_shape=None):\n",
    "    paths = Path(dir_path).glob('**/*.jpg')\n",
    "    deleted  = 0\n",
    "    for path in paths:\n",
    "        try:\n",
    "            # Make sure we can decode the image\n",
    "            im = tf.io.read_file(str(path.resolve()))\n",
    "            im = tf.image.decode_jpeg(im)\n",
    "\n",
    "            # Remove grayscale images \n",
    "            shape = im.shape\n",
    "            if shape[2] < 3:\n",
    "                path.unlink()\n",
    "                deleted += 1\n",
    "\n",
    "            # Remove small images\n",
    "            if min_shape is not None:\n",
    "                if shape[0] < min_shape[0] or shape[1] < min_shape[1]:\n",
    "                    path.unlink()\n",
    "                    deleted += 1\n",
    "        except Exception as e:\n",
    "            path.unlink()\n",
    "            deleted += 1\n",
    "    return deleted\n",
    "\n",
    "for dir in CONTENT_DIRS:\n",
    "    deleted = clean(dir)\n",
    "print(f'#Deleted content images: {deleted}')\n",
    "\n",
    "for dir in STYLE_DIRS:\n",
    "    deleted = clean(dir)\n",
    "print(f'#Deleted style images: {deleted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path, init_shape=(448, 448)):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, init_shape)\n",
    "    image = tf.image.random_crop(image, size=IMG_SHAPE)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    # Convert image from RGB to BGR, then zero-center each color channel with\n",
    "    # respect to the ImageNet dataset, without scaling.\n",
    "    image = image[..., ::-1] # RGB to BGR\n",
    "    image -= (103.939, 116.779, 123.68) # BGR means\n",
    "    return image\n",
    "\n",
    "def np_image(image):\n",
    "    image += (103.939, 116.779, 123.68) # BGR means\n",
    "    image = image[..., ::-1] # BGR to RGB\n",
    "    image = tf.clip_by_value(image, 0, 255)\n",
    "    image = tf.cast(image, dtype='uint8')\n",
    "    return image.numpy()\n",
    "\n",
    "def build_dataset(num_gpus=1):\n",
    "    c_paths = []\n",
    "    for c_dir in CONTENT_DIRS:\n",
    "        c_paths += Path(c_dir).glob('*.jpg')\n",
    "    c_paths = [str(path.resolve()) for path in c_paths]\n",
    "    s_paths = []\n",
    "    for s_dir in STYLE_DIRS:\n",
    "        s_paths += Path(s_dir).glob('*.jpg')\n",
    "    s_paths = [str(path.resolve()) for path in s_paths]\n",
    "    print(f'Building dataset from {len(c_paths):,} content images and {len(s_paths):,} style images... ', end='')\n",
    "\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    c_ds = tf.data.Dataset.from_tensor_slices(c_paths)\n",
    "    c_ds = c_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    c_ds = c_ds.repeat()\n",
    "    c_ds = c_ds.shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "\n",
    "    s_ds = tf.data.Dataset.from_tensor_slices(s_paths)\n",
    "    s_ds = s_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    s_ds = s_ds.repeat()\n",
    "    s_ds = s_ds.shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "\n",
    "    ds = tf.data.Dataset.zip((c_ds, s_ds))\n",
    "    ds = ds.batch(BATCH_SIZE * num_gpus)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    print('done')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = build_dataset()\n",
    "c_batch, s_batch = next(iter(ds.take(1)))\n",
    "\n",
    "print('Content batch shape:', c_batch.shape)\n",
    "print('Style batch shape:', s_batch.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np_image(c_batch[0]))\n",
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Content')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np_image(s_batch[0]))\n",
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Style')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Instance Normalization\n",
    "\n",
    "$$\\text{AdaIN}(x,\\,y) = \\sigma(y)\\,(\\cfrac{x - \\mu(x)}{\\sigma(x)}) + \\mu(y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(tf.keras.layers.Layer):\n",
    "    # TODO\n",
    "    def __init__(self, name):\n",
    "        super(AdaIN, self).__init__() \n",
    "        self.epsilon = 1e-6\n",
    "        \n",
    "    def call(self, input):\n",
    "        (x, y) = input\n",
    "        mean_x, variance_x = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
    "        std_x = tf.sqrt(variance_x + self.epsilon)\n",
    "        \n",
    "        mean_y, variance_y = tf.nn.moments(y, axes=[1, 2], keepdims=True)\n",
    "        std_y = tf.sqrt(variance_y + self.epsilon)        \n",
    "        \n",
    "        return std_y * ((x - mean_x) /std_x) + mean_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArbitraryStyleTransferNet(tf.keras.Model):\n",
    "    CONTENT_LAYER = 'block4_conv1'\n",
    "    STYLE_LAYERS = ('block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1')\n",
    "\n",
    "    @staticmethod\n",
    "    def declare_decoder():\n",
    "        a_input = tf.keras.Input(shape=(28, 28, 512), name='input_adain')\n",
    "\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(a_input)\n",
    "        h = tf.keras.layers.UpSampling2D(2)(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(128, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.UpSampling2D(2)(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(128, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(64, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.UpSampling2D(2)(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(64, 3, padding='same', activation='relu')(h)\n",
    "        output = tf.keras.layers.Conv2DTranspose(3, 3, padding='same')(h)\n",
    "\n",
    "        return tf.keras.Model(inputs=a_input, outputs=output, name='decoder')\n",
    "  \n",
    "    def __init__(self,\n",
    "                 img_shape=(224, 224, 3),\n",
    "                 content_loss_weight=1,\n",
    "                 style_loss_weight=10,\n",
    "                 name='arbitrary_style_transfer_net',\n",
    "                 **kwargs):\n",
    "        super(ArbitraryStyleTransferNet, self).__init__(name=name, **kwargs)\n",
    "\n",
    "        self.img_shape = img_shape\n",
    "        self.content_loss_weight = content_loss_weight\n",
    "        self.style_loss_weight = style_loss_weight\n",
    "        \n",
    "        vgg19 = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=img_shape)\n",
    "        vgg19.trainable = False\n",
    "\n",
    "        c_output = [vgg19.get_layer(ArbitraryStyleTransferNet.CONTENT_LAYER).output]\n",
    "        s_outputs = [vgg19.get_layer(name).output for name in ArbitraryStyleTransferNet.STYLE_LAYERS]\n",
    "        self.vgg19 = tf.keras.Model(inputs=vgg19.input, outputs=c_output+s_outputs, name='vgg19')\n",
    "        self.vgg19.trainable = False\n",
    "\n",
    "        self.adain = AdaIN(name='adain')\n",
    "        self.decoder = ArbitraryStyleTransferNet.declare_decoder()\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        c_batch, s_batch = inputs\n",
    "\n",
    "        c_enc = self.vgg19(c_batch)\n",
    "        c_enc_c = c_enc[0]\n",
    "\n",
    "        s_enc = self.vgg19(s_batch)\n",
    "        s_enc_c = s_enc[0]\n",
    "        s_enc_s = s_enc[1:] \n",
    "        \n",
    "        # normalized_c is the output of AdaIN layer\n",
    "        normalized_c = self.adain((c_enc_c, s_enc_c))\n",
    "        output = self.decoder(normalized_c)\n",
    "\n",
    "        # Calculate loss\n",
    "        out_enc = self.vgg19(output)\n",
    "        out_enc_c = out_enc[0]\n",
    "        out_enc_s = out_enc[1:]\n",
    "\n",
    "        loss_c = tf.reduce_mean(tf.math.squared_difference(out_enc_c, normalized_c))\n",
    "        self.add_loss(self.content_loss_weight * loss_c)\n",
    "        \n",
    "        loss_s = 0\n",
    "        for o, s in zip(out_enc_s, s_enc_s):    \n",
    "            o_mean, o_var = tf.nn.moments(o, axes=(1,2), keepdims=True)\n",
    "            o_std = tf.sqrt(o_var + self.adain.epsilon)\n",
    "\n",
    "            s_mean, s_var = tf.nn.moments(s, axes=(1,2), keepdims=True)\n",
    "            s_std = tf.sqrt(s_var + self.adain.epsilon)\n",
    "\n",
    "            loss_mean = tf.reduce_mean(tf.math.squared_difference(o_mean, s_mean))\n",
    "            loss_std = tf.reduce_mean(tf.math.squared_difference(o_std, s_std))\n",
    "\n",
    "            loss_s += loss_mean + loss_std\n",
    "        self.add_loss(self.style_loss_weight * loss_s)\n",
    "\n",
    "        return output, c_enc_c, normalized_c, out_enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "def plot_outputs(outputs, captions=None, col=5):\n",
    "    row = len(outputs)\n",
    "    plt.figure(figsize=(3*col, 3*row))\n",
    "    for i in range(col):\n",
    "        for j in range(row):\n",
    "            plt.subplot(row, col, j*col+i+1)\n",
    "            plt.imshow(np_image(outputs[j][i,...,:3]))\n",
    "            plt.grid(False)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            if captions is not None:\n",
    "                plt.xlabel(captions[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = build_dataset()\n",
    "model = ArbitraryStyleTransferNet(img_shape=IMG_SHAPE)\n",
    "\n",
    "c_batch, s_batch = next(iter(ds.take(1)))\n",
    "print(f'Input shape: ({c_batch.shape}, {s_batch.shape})')\n",
    "output, *_ = model((c_batch, s_batch))\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Init. content loss: {model.losses[0]:,.2f}, style loss: {model.losses[1]:,.2f}')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "c_loss_metric, s_loss_metric = tf.keras.metrics.Mean(), tf.keras.metrics.Mean()\n",
    "\n",
    "CKP_DIR = 'checkpoints'\n",
    "init_epoch = 1\n",
    "\n",
    "ckp = tf.train.latest_checkpoint(CKP_DIR)\n",
    "if ckp:\n",
    "    model.load_weights(ckp)\n",
    "    init_epoch = int(ckp.split('_')[-1]) + 1\n",
    "    print(f'Resume training from epoch {init_epoch-1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model(inputs)\n",
    "        c_loss, s_loss = model.losses\n",
    "        loss = c_loss + s_loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    c_loss_metric(c_loss)\n",
    "    s_loss_metric(s_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, init_epoch):\n",
    "    for epoch in range(init_epoch, EPOCHS+1):\n",
    "        print(f'Epoch {epoch:>2}/{EPOCHS}')\n",
    "        for step, inputs in enumerate(ds.take(STEPS_PER_EPOCH)):\n",
    "            train_step(inputs)\n",
    "            print(f'{step+1:>5}/{STEPS_PER_EPOCH} - loss: {c_loss_metric.result()+s_loss_metric.result():,.2f} - content loss: {c_loss_metric.result():,.2f} - style loss: {s_loss_metric.result():,.2f}', end='\\r') \n",
    "\n",
    "        print()\n",
    "        model.save_weights(os.path.join(CKP_DIR, f'ckpt_{epoch}'))\n",
    "        c_loss_metric.reset_states()\n",
    "        s_loss_metric.reset_states()\n",
    "\n",
    "        output, c_enc_c, normalized_c, out_enc_c = model((c_batch, s_batch))\n",
    "        plot_outputs((s_batch, c_batch, output, c_enc_c, normalized_c, out_enc_c), \n",
    "                     ('Style', 'Content', 'Trans', 'Content Enc', 'Normalized', 'Trans Enc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(ds, init_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKP_DIR = 'checkpoints/ckpt_20'\n",
    "\n",
    "model = ArbitraryStyleTransferNet(img_shape=IMG_SHAPE)\n",
    "model.load_weights(CKP_DIR)\n",
    "\n",
    "ds = build_dataset()\n",
    "\n",
    "for idx, (c_batch, s_batch) in enumerate(ds):\n",
    "    if idx > 1:\n",
    "        break\n",
    "    output, c_enc_c, normalized_c, out_enc_c = model((c_batch, s_batch))\n",
    "    print('Recovered loss:', tf.reduce_sum(model.losses).numpy())\n",
    "\n",
    "    plot_outputs((s_batch, c_batch, output), ('Style', 'Content', 'Trans'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTHU Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_example(path, init_shape=(IMG_SHAPE[0], IMG_SHAPE[1])):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, init_shape)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    # Convert image from RGB to BGR, then zero-center each color channel with\n",
    "    # respect to the ImageNet dataset, without scaling.\n",
    "    image = image[..., ::-1] # RGB to BGR\n",
    "    image -= (103.939, 116.779, 123.68) # BGR means\n",
    "    return image\n",
    "\n",
    "def nthu_example(num_gpus=1):\n",
    "    c_paths = ['./dataset/content_nthu.jpg']\n",
    "    \n",
    "    s_paths = []\n",
    "    for s_dir in STYLE_DIRS:\n",
    "        s_paths += Path(s_dir).glob('*.jpg')\n",
    "    s_paths = [str(path.resolve()) for path in s_paths]\n",
    "    print(f'Building dataset from {len(c_paths):,} content images and {len(s_paths):,} style images... ', end='')\n",
    "    \n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "    c_ds = tf.data.Dataset.from_tensor_slices(c_paths)\n",
    "    c_ds = c_ds.map(preprocess_example, num_parallel_calls=AUTOTUNE)\n",
    "    c_ds = c_ds.repeat()\n",
    "    c_ds = c_ds.shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "    \n",
    "    s_ds = tf.data.Dataset.from_tensor_slices(s_paths)\n",
    "    s_ds = s_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    s_ds = s_ds.repeat()\n",
    "    s_ds = s_ds.shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "    \n",
    "    ds = tf.data.Dataset.zip((c_ds, s_ds))\n",
    "    ds = ds.batch(BATCH_SIZE * num_gpus)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    print('done')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = nthu_example()\n",
    "outputs = []\n",
    "\n",
    "for idx, (c_batch, s_batch) in enumerate(ds):\n",
    "    if idx > 4:\n",
    "        break\n",
    "    output, c_enc_c, normalized_c, out_enc_c = model((c_batch, s_batch))\n",
    "    outputs.append(output)\n",
    "    \n",
    "plot_outputs((outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Models\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/11-2_Visualization_Style-Transfer/figs/tensorflow_family.png)\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/11-2_Visualization_Style-Transfer/figs/graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inside checkpoint\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/11-2_Visualization_Style-Transfer/figs/checkpoints.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_labels = train_labels[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
    "test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.d2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = MyModel()\n",
    "model.build(input_shape=(None, 28, 28))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint callback usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "# Checkpoint path and its name\n",
    "CKP_DIR_SAVE_CALLBACKS = './checkpoints_save_callbacks/ckpt-{epoch}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(CKP_DIR_SAVE_CALLBACKS)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Create a callback that saves the model's weights every 1 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=CKP_DIR_SAVE_CALLBACKS, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    period=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "model.fit(train_images, \n",
    "          train_labels,\n",
    "          epochs=EPOCHS, \n",
    "          callbacks=[cp_callback],\n",
    "          validation_data=(test_images,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance\n",
    "model = MyModel()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate the model\n",
    "loss, acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved weights\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model.load_weights(latest)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = MyModel()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKP_DIR_SAVE_WEIGHTS = './checkpoints_save_weights'\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {:0}, Loss: {:.2f}, Accuracy: {:.2f}, Test Loss: {:.2f}, Test Accuracy: {:.2f}'\n",
    "    print (template.format(epoch+1,\n",
    "                           train_loss.result(),\n",
    "                           train_accuracy.result()*100,\n",
    "                           test_loss.result(),\n",
    "                           test_accuracy.result()*100))\n",
    "    \n",
    "    # Use Model.save_weights during training\n",
    "    # You can modify the saving frequency by simply using \"if epoch == ?, then save\"\n",
    "    print(\"Saved checkpoint for step {}: {}\".format(int(epoch+1), CKP_DIR_SAVE_WEIGHTS + f'/ckpt-{epoch+1}'))\n",
    "    model.save_weights(os.path.join(CKP_DIR_SAVE_WEIGHTS, f'ckpt-{epoch}'))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "template = 'Test Loss: {:.2f}, Test Accuracy: {:.2f}'\n",
    "print (template.format(test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the weights\n",
    "model.load_weights('checkpoints_save_weights/ckpt-4')\n",
    "\n",
    "for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "template = 'Test Loss: {:.2f}, Test Accuracy: {:.2f}'\n",
    "print (template.format(test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance\n",
    "tf.keras.backend.clear_session()\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKP_DIR_SAVE_CHECKPOINTS = './checkpoints_save_checkpoints'\n",
    "\n",
    "# Place the models and optimizers you want to store \n",
    "# as the arguments of tf.train.Checkpoint\n",
    "# You can store several different models and optimizers at the same time\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, CKP_DIR_SAVE_CHECKPOINTS, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "        \n",
    "    template = 'Epoch {:0}, Loss: {:.2f}, Accuracy: {:.2f}, Test Loss: {:.2f}, Test Accuracy: {:.2f}'\n",
    "    print (template.format(epoch+1,\n",
    "                           train_loss.result(),\n",
    "                           train_accuracy.result()*100,\n",
    "                           test_loss.result(),\n",
    "                           test_accuracy.result()*100))\n",
    "    \n",
    "    # save checkpoint for each epoch\n",
    "    if int(ckpt.step) % 1 == 0:\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "    \n",
    "    ckpt.step.assign_add(1)\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "template = 'Test Loss: {:.2f}, Test Accuracy: {:.2f}'\n",
    "print (template.format(test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load checkpoints back to our new model, you have to create another \n",
    "# \"tf.train.Checkpoint\" for new model and optimizer\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, CKP_DIR_SAVE_CHECKPOINTS, max_to_keep=3)\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "\n",
    "for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "template = 'Test Loss: {:.2f}, Test Accuracy: {:.2f}'\n",
    "print (template.format(test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
